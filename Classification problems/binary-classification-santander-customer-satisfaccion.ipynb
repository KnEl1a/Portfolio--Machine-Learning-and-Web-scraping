{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4986,"databundleVersionId":860641,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This Kaggle challenge, proposed by Santander Bank, focuses on identifying dissatisfied customers early on. The idea is to predict the probability of a customer being dissatisfied (TARGET = 1) or satisfied (TARGET = 0) using a dataset with hundreds of anonymized numerical features. This would allow the bank to take proactive steps to improve customer satisfaction before they decide to leave.\n\n### Data:\n- **train.csv**: Contains the training set, including the \"TARGET\" column, which indicates whether the customer is dissatisfied (1) or satisfied (0).\n- **test.csv**: Contains the test set, but without the \"TARGET\" column.\n- **sample_submission.csv**: A sample file showing the correct format for predictions.\n\n### Objective:\nPredict the probability that customers in the test set are dissatisfied (TARGET = 1).\n\n### Evaluation:\nPredictions are evaluated using the area under the ROC curve (AUC), which measures how well the model distinguishes between satisfied and dissatisfied customers. For each ID in the test set, you must predict the probability of the customer being dissatisfied.\n\nIn summary, the approach is to use anonymized numerical data to build a model that predicts customer dissatisfaction, evaluating the model's performance through AUC.\n\n<!-- Este desafío de Kaggle, planteado por el banco Santander, se enfoca en identificar a los clientes insatisfechos de forma temprana. La idea es predecir la probabilidad de que un cliente esté insatisfecho (TARGET = 1) o satisfecho (TARGET = 0) usando un conjunto de datos con cientos de características numéricas anonimizadas. Esto permitiría al banco tomar medidas proactivas para mejorar la satisfacción del cliente antes de que decidan irse.\n\n### Datos:\n- **train.csv**: Contiene el conjunto de entrenamiento, incluyendo la columna \"TARGET\", que indica si el cliente está insatisfecho (1) o satisfecho (0).\n- **test.csv**: Contiene el conjunto de prueba, pero sin la columna \"TARGET\".\n- **sample_submission.csv**: Es un archivo de ejemplo que muestra el formato correcto para las predicciones.\n\n### Objetivo:\nPredecir la probabilidad de que los clientes en el conjunto de prueba estén insatisfechos (TARGET = 1).\n\n### Evaluación:\nLas predicciones se evalúan usando el área bajo la curva ROC (AUC), que mide qué tan bien el modelo separa a los clientes satisfechos de los insatisfechos. Para cada ID del conjunto de prueba, se debe predecir la probabilidad de que el cliente esté insatisfecho.\n\nEn resumen, el enfoque es usar datos numéricos anonimizados para crear un modelo que prediga la probabilidad de insatisfacción de los clientes, evaluando el rendimiento del modelo a través del AUC. -->","metadata":{}},{"cell_type":"markdown","source":"-----\n\nThis code is designed to solve a classification problem using the XGBoost algorithm, optimizing model performance through preprocessing, dimensionality reduction, and feature selection techniques. Below is a description of each part of the code:\n\n1. **Library Imports**:\n   - `numpy` and `pandas` are standard libraries for data handling.\n   - `matplotlib.pyplot` is used for visualizing graphs.\n   - `xgboost` is the main library for the XGBoost classification algorithm.\n   - `sklearn` provides tools for cross-validation and feature selection.\n\n   ```python\n   import numpy as np\n   import pandas as pd\n   import matplotlib.pyplot as plt\n   import matplotlib\n   matplotlib.use(\"Agg\")  # Needed to save plots instead of displaying them\n   from sklearn import cross_validation\n   import xgboost as xgb\n   from sklearn.metrics import roc_auc_score\n   ```\n\n2. **Data Loading**:\n   - Training and test data are loaded from CSV files.\n   - `index_col=0` is set to use the first column as the index for each row.\n\n   ```python\n   training = pd.read_csv(\"../input/train.csv\", index_col=0)\n   test = pd.read_csv(\"../input/test.csv\", index_col=0)\n   ```\n\n3. **Data Preprocessing**:\n   - Anomalous values in the `var3` column are replaced with the most common value (2), and new features are added.\n   - A new column that counts zeros per row is added as a feature, and PCA (Principal Component Analysis) is applied to reduce dimensionality.\n\n   ```python\n   training = training.replace(-999999, 2)\n   X = training.iloc[:, :-1]  # Features (all columns except TARGET)\n   y = training.TARGET  # Labels\n\n   X['n0'] = (X == 0).sum(axis=1)  # New feature counting zeros per row\n\n   from sklearn.preprocessing import normalize\n   from sklearn.decomposition import PCA\n   X_normalized = normalize(X, axis=0)\n   pca = PCA(n_components=2)\n   X_pca = pca.fit_transform(X_normalized)\n   X['PCA1'] = X_pca[:, 0]\n   X['PCA2'] = X_pca[:, 1]\n   ```\n\n4. **Feature Selection**:\n   - Relevant features are selected using two methods: `chi2` and `f_classif`, both from `sklearn`. Only features selected by both methods are kept.\n\n   ```python\n   from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n   from sklearn.preprocessing import Binarizer, scale\n\n   X_bin = Binarizer().fit_transform(scale(X))\n   selectChi2 = SelectPercentile(chi2, percentile=75).fit(X_bin, y)\n   selectF_classif = SelectPercentile(f_classif, percentile=75).fit(X, y)\n   \n   chi2_selected = selectChi2.get_support()\n   f_classif_selected = selectF_classif.get_support()\n   selected = chi2_selected & f_classif_selected\n   features = [f for f, s in zip(X.columns, selected) if s]\n   ```\n\n5. **Dataset Splitting**:\n   - The data is split into training and test sets using stratified cross-validation to maintain class proportions.\n\n   ```python\n   X_train, X_test, y_train, y_test = cross_validation.train_test_split(X[features], y, random_state=1301, stratify=y, test_size=0.4)\n   ```\n\n6. **XGBoost Model Setup and Training**:\n   - Several hyperparameters are tuned, and the model is trained using internal validation to measure AUC (Area Under the ROC Curve).\n   - Hyperparameters such as `max_depth`, `subsample`, and `learning_rate` are fine-tuned to improve model performance.\n\n   ```python\n   clf = xgb.XGBClassifier(missing=9999999999, max_depth=5, n_estimators=1000, learning_rate=0.1, nthread=4, subsample=1.0, colsample_bytree=0.5, min_child_weight=3, scale_pos_weight=ratio, seed=1301)\n   clf.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"auc\", eval_set=[(X_train, y_train), (X_test, y_test)])\n   ```\n\n7. **Prediction and Evaluation**:\n   - Predictions are made on the test set, and AUC is calculated to evaluate overall model performance.\n\n   ```python\n   print('Overall AUC:', roc_auc_score(y, clf.predict_proba(X[features], ntree_limit=clf.best_iteration)[:,1]))\n   ```\n\n8. **Test Data Preparation and Final Prediction**:\n   - The same PCA and normalization transformations are applied to the test data before making predictions.\n   - The prediction is saved to a CSV file for submission.\n\n   ```python\n   test['n0'] = (test == 0).sum(axis=1)\n   test_normalized = normalize(test, axis=0)\n   test_pca = pca.fit_transform(test_normalized)\n   test['PCA1'] = test_pca[:, 0]\n   test['PCA2'] = test_pca[:, 1]\n   sel_test = test[features]\n   y_pred = clf.predict_proba(sel_test, ntree_limit=clf.best_iteration)\n   submission = pd.DataFrame({\"ID\": test.index, \"TARGET\": y_pred[:, 1]})\n   submission.to_csv(\"submission.csv\", index=False)\n   ```\n\n9. **Feature Importance**:\n   - Finally, the 15 most important features according to the XGBoost model are visualized, and the plot is saved.\n\n   ```python\n   ts = pd.Series(clf.booster().get_fscore()).sort_values()[-15:]\n   featp = ts.plot(kind='barh', figsize=(6, 10))\n   plt.title('XGBoost Feature Importance')\n   fig_featp = featp.get_figure()\n   fig_featp.savefig('feature_importance_xgb.png', bbox_inches='tight', pad_inches=1)\n   ```\n\n<!-- Este código está diseñado para resolver un problema de clasificación utilizando el algoritmo XGBoost, optimizando el rendimiento del modelo mediante técnicas de preprocesamiento, reducción de dimensionalidad y selección de características. A continuación, se describe cada parte del código:\n\n1. **Importación de librerías**:\n   - `numpy` y `pandas` son librerías estándar para manejo de datos.\n   - `matplotlib.pyplot` se utiliza para la visualización de gráficos.\n   - `xgboost` es la librería principal para el algoritmo de clasificación XGBoost.\n   - `sklearn` proporciona herramientas para la validación cruzada y selección de características.\n   \n   ```python\n   import numpy as np\n   import pandas as pd\n   import matplotlib.pyplot as plt\n   import matplotlib\n   matplotlib.use(\"Agg\")  # Necesario para guardar gráficos en lugar de mostrarlos\n   from sklearn import cross_validation\n   import xgboost as xgb\n   from sklearn.metrics import roc_auc_score\n   ```\n\n2. **Carga de datos**:\n   - Se cargan los datos de entrenamiento y prueba desde archivos CSV.\n   - Se define `index_col=0` para que la primera columna sea el índice de cada fila.\n\n   ```python\n   training = pd.read_csv(\"../input/train.csv\", index_col=0)\n   test = pd.read_csv(\"../input/test.csv\", index_col=0)\n   ```\n\n3. **Preprocesamiento de datos**:\n   - Se reemplazan valores anómalos en la columna `var3` con el valor más común (2), y se agregan nuevas características.\n   - Se añade una columna que cuenta los ceros por fila como una característica adicional, y se aplica PCA (análisis de componentes principales) para reducir la dimensionalidad.\n\n   ```python\n   training = training.replace(-999999, 2)\n   X = training.iloc[:, :-1]  # Características (todas las columnas excepto TARGET)\n   y = training.TARGET  # Etiquetas\n\n   X['n0'] = (X == 0).sum(axis=1)  # Nueva característica que cuenta ceros por fila\n\n   from sklearn.preprocessing import normalize\n   from sklearn.decomposition import PCA\n   X_normalized = normalize(X, axis=0)\n   pca = PCA(n_components=2)\n   X_pca = pca.fit_transform(X_normalized)\n   X['PCA1'] = X_pca[:, 0]\n   X['PCA2'] = X_pca[:, 1]\n   ```\n\n4. **Selección de características**:\n   - Se seleccionan características relevantes utilizando dos métodos: `chi2` y `f_classif`, ambos de `sklearn`. Luego se eligen solo las características seleccionadas por ambos métodos.\n\n   ```python\n   from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n   from sklearn.preprocessing import Binarizer, scale\n\n   X_bin = Binarizer().fit_transform(scale(X))\n   selectChi2 = SelectPercentile(chi2, percentile=75).fit(X_bin, y)\n   selectF_classif = SelectPercentile(f_classif, percentile=75).fit(X, y)\n   \n   chi2_selected = selectChi2.get_support()\n   f_classif_selected = selectF_classif.get_support()\n   selected = chi2_selected & f_classif_selected\n   features = [f for f, s in zip(X.columns, selected) if s]\n   ```\n\n5. **División del conjunto de datos**:\n   - Se dividen los datos en conjunto de entrenamiento y prueba utilizando validación cruzada estratificada para mantener la proporción de las clases.\n\n   ```python\n   X_train, X_test, y_train, y_test = cross_validation.train_test_split(X[features], y, random_state=1301, stratify=y, test_size=0.4)\n   ```\n\n6. **Configuración y entrenamiento del modelo XGBoost**:\n   - Se ajustan varios hiperparámetros y se entrena el modelo usando validación interna para medir el AUC (Área bajo la curva ROC).\n   - Se realiza ajuste fino de hiperparámetros, como `max_depth`, `subsample`, y `learning_rate` para mejorar el rendimiento del modelo.\n\n   ```python\n   clf = xgb.XGBClassifier(missing=9999999999, max_depth=5, n_estimators=1000, learning_rate=0.1, nthread=4, subsample=1.0, colsample_bytree=0.5, min_child_weight=3, scale_pos_weight=ratio, seed=1301)\n   clf.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"auc\", eval_set=[(X_train, y_train), (X_test, y_test)])\n   ```\n\n7. **Predicción y evaluación**:\n   - Se predice sobre el conjunto de prueba y se calcula el AUC para evaluar el rendimiento global del modelo.\n\n   ```python\n   print('Overall AUC:', roc_auc_score(y, clf.predict_proba(X[features], ntree_limit=clf.best_iteration)[:,1]))\n   ```\n\n8. **Preparación de datos de prueba y generación de la predicción final**:\n   - Se aplican las mismas transformaciones de PCA y normalización a los datos de prueba antes de realizar la predicción.\n   - Se guarda la predicción en un archivo CSV para su envío.\n\n   ```python\n   test['n0'] = (test == 0).sum(axis=1)\n   test_normalized = normalize(test, axis=0)\n   test_pca = pca.fit_transform(test_normalized)\n   test['PCA1'] = test_pca[:, 0]\n   test['PCA2'] = test_pca[:, 1]\n   sel_test = test[features]\n   y_pred = clf.predict_proba(sel_test, ntree_limit=clf.best_iteration)\n   submission = pd.DataFrame({\"ID\": test.index, \"TARGET\": y_pred[:, 1]})\n   submission.to_csv(\"submission.csv\", index=False)\n   ```\n\n9. **Importancia de características**:\n   - Finalmente, se visualizan las 15 características más importantes según el modelo XGBoost y se guarda la gráfica.\n\n   ```python\n   ts = pd.Series(clf.booster().get_fscore()).sort_values()[-15:]\n   featp = ts.plot(kind='barh', figsize=(6, 10))\n   plt.title('XGBoost Feature Importance')\n   fig_featp = featp.get_figure()\n   fig_featp.savefig('feature_importance_xgb.png', bbox_inches='tight', pad_inches=1)\n   ```\n -->\n \n \n ------","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use(\"Agg\") #Needed to save figures","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:46:04.164100Z","iopub.execute_input":"2024-09-07T22:46:04.164547Z","iopub.status.idle":"2024-09-07T22:46:04.170333Z","shell.execute_reply.started":"2024-09-07T22:46:04.164508Z","shell.execute_reply":"2024-09-07T22:46:04.168907Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn import cross_validation\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntraining = pd.read_csv(\"/kaggle/input/santander-customer-satisfaction/train.csv\", index_col=0)\ntest = pd.read_csv(\"/kaggle/input/santander-customer-satisfaction/test.csv\", index_col=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:46:28.582001Z","iopub.execute_input":"2024-09-07T22:46:28.582450Z","iopub.status.idle":"2024-09-07T22:46:34.363499Z","shell.execute_reply.started":"2024-09-07T22:46:28.582396Z","shell.execute_reply":"2024-09-07T22:46:34.362411Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"training","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:46:41.560350Z","iopub.execute_input":"2024-09-07T22:46:41.561530Z","iopub.status.idle":"2024-09-07T22:46:41.618076Z","shell.execute_reply.started":"2024-09-07T22:46:41.561473Z","shell.execute_reply":"2024-09-07T22:46:41.617021Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"        var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\nID                                                                 \n1          2     23                 0.0                      0.0   \n3          2     34                 0.0                      0.0   \n4          2     23                 0.0                      0.0   \n8          2     37                 0.0                    195.0   \n10         2     39                 0.0                      0.0   \n...      ...    ...                 ...                      ...   \n151829     2     48                 0.0                      0.0   \n151830     2     39                 0.0                      0.0   \n151835     2     23                 0.0                      0.0   \n151836     2     25                 0.0                      0.0   \n151838     2     46                 0.0                      0.0   \n\n        imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  \\\nID                                                         \n1                           0.0                      0.0   \n3                           0.0                      0.0   \n4                           0.0                      0.0   \n8                         195.0                      0.0   \n10                          0.0                      0.0   \n...                         ...                      ...   \n151829                      0.0                      0.0   \n151830                      0.0                      0.0   \n151835                      0.0                      0.0   \n151836                      0.0                      0.0   \n151838                      0.0                      0.0   \n\n        imp_op_var40_comer_ult3  imp_op_var40_efect_ult1  \\\nID                                                         \n1                           0.0                      0.0   \n3                           0.0                      0.0   \n4                           0.0                      0.0   \n8                           0.0                      0.0   \n10                          0.0                      0.0   \n...                         ...                      ...   \n151829                      0.0                      0.0   \n151830                      0.0                      0.0   \n151835                      0.0                      0.0   \n151836                      0.0                      0.0   \n151838                      0.0                      0.0   \n\n        imp_op_var40_efect_ult3  imp_op_var40_ult1  ...  \\\nID                                                  ...   \n1                           0.0                0.0  ...   \n3                           0.0                0.0  ...   \n4                           0.0                0.0  ...   \n8                           0.0                0.0  ...   \n10                          0.0                0.0  ...   \n...                         ...                ...  ...   \n151829                      0.0                0.0  ...   \n151830                      0.0                0.0  ...   \n151835                      0.0                0.0  ...   \n151836                      0.0                0.0  ...   \n151838                      0.0                0.0  ...   \n\n        saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\nID                                                         \n1                           0.0                      0.0   \n3                           0.0                      0.0   \n4                           0.0                      0.0   \n8                           0.0                      0.0   \n10                          0.0                      0.0   \n...                         ...                      ...   \n151829                      0.0                      0.0   \n151830                      0.0                      0.0   \n151835                      0.0                      0.0   \n151836                      0.0                      0.0   \n151838                      0.0                      0.0   \n\n        saldo_medio_var33_ult1  saldo_medio_var33_ult3  \\\nID                                                       \n1                          0.0                     0.0   \n3                          0.0                     0.0   \n4                          0.0                     0.0   \n8                          0.0                     0.0   \n10                         0.0                     0.0   \n...                        ...                     ...   \n151829                     0.0                     0.0   \n151830                     0.0                     0.0   \n151835                     0.0                     0.0   \n151836                     0.0                     0.0   \n151838                     0.0                     0.0   \n\n        saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\nID                                                         \n1                           0.0                      0.0   \n3                           0.0                      0.0   \n4                           0.0                      0.0   \n8                           0.0                      0.0   \n10                          0.0                      0.0   \n...                         ...                      ...   \n151829                      0.0                      0.0   \n151830                      0.0                      0.0   \n151835                      0.0                      0.0   \n151836                      0.0                      0.0   \n151838                      0.0                      0.0   \n\n        saldo_medio_var44_ult1  saldo_medio_var44_ult3          var38  TARGET  \nID                                                                             \n1                          0.0                     0.0   39205.170000       0  \n3                          0.0                     0.0   49278.030000       0  \n4                          0.0                     0.0   67333.770000       0  \n8                          0.0                     0.0   64007.970000       0  \n10                         0.0                     0.0  117310.979016       0  \n...                        ...                     ...            ...     ...  \n151829                     0.0                     0.0   60926.490000       0  \n151830                     0.0                     0.0  118634.520000       0  \n151835                     0.0                     0.0   74028.150000       0  \n151836                     0.0                     0.0   84278.160000       0  \n151838                     0.0                     0.0  117310.979016       0  \n\n[76020 rows x 370 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var3</th>\n      <th>var15</th>\n      <th>imp_ent_var16_ult1</th>\n      <th>imp_op_var39_comer_ult1</th>\n      <th>imp_op_var39_comer_ult3</th>\n      <th>imp_op_var40_comer_ult1</th>\n      <th>imp_op_var40_comer_ult3</th>\n      <th>imp_op_var40_efect_ult1</th>\n      <th>imp_op_var40_efect_ult3</th>\n      <th>imp_op_var40_ult1</th>\n      <th>...</th>\n      <th>saldo_medio_var33_hace2</th>\n      <th>saldo_medio_var33_hace3</th>\n      <th>saldo_medio_var33_ult1</th>\n      <th>saldo_medio_var33_ult3</th>\n      <th>saldo_medio_var44_hace2</th>\n      <th>saldo_medio_var44_hace3</th>\n      <th>saldo_medio_var44_ult1</th>\n      <th>saldo_medio_var44_ult3</th>\n      <th>var38</th>\n      <th>TARGET</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>23</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>39205.170000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>34</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>49278.030000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>23</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>67333.770000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>37</td>\n      <td>0.0</td>\n      <td>195.0</td>\n      <td>195.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>64007.970000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2</td>\n      <td>39</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>117310.979016</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>151829</th>\n      <td>2</td>\n      <td>48</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>60926.490000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>151830</th>\n      <td>2</td>\n      <td>39</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>118634.520000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>151835</th>\n      <td>2</td>\n      <td>23</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>74028.150000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>151836</th>\n      <td>2</td>\n      <td>25</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>84278.160000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>151838</th>\n      <td>2</td>\n      <td>46</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>117310.979016</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>76020 rows × 370 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(training.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:47:04.879000Z","iopub.execute_input":"2024-09-07T22:47:04.879499Z","iopub.status.idle":"2024-09-07T22:47:04.885338Z","shell.execute_reply.started":"2024-09-07T22:47:04.879456Z","shell.execute_reply":"2024-09-07T22:47:04.883987Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(76020, 370)\n(75818, 369)\n","output_type":"stream"}]},{"cell_type":"code","source":"training.describe()","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:47:27.508441Z","iopub.execute_input":"2024-09-07T22:47:27.508872Z","iopub.status.idle":"2024-09-07T22:47:28.810976Z","shell.execute_reply.started":"2024-09-07T22:47:27.508833Z","shell.execute_reply":"2024-09-07T22:47:28.809725Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                var3         var15  imp_ent_var16_ult1  \\\ncount   76020.000000  76020.000000        76020.000000   \nmean    -1523.199277     33.212865           86.208265   \nstd     39033.462364     12.956486         1614.757313   \nmin   -999999.000000      5.000000            0.000000   \n25%         2.000000     23.000000            0.000000   \n50%         2.000000     28.000000            0.000000   \n75%         2.000000     40.000000            0.000000   \nmax       238.000000    105.000000       210000.000000   \n\n       imp_op_var39_comer_ult1  imp_op_var39_comer_ult3  \\\ncount             76020.000000             76020.000000   \nmean                 72.363067               119.529632   \nstd                 339.315831               546.266294   \nmin                   0.000000                 0.000000   \n25%                   0.000000                 0.000000   \n50%                   0.000000                 0.000000   \n75%                   0.000000                 0.000000   \nmax               12888.030000             21024.810000   \n\n       imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\ncount             76020.000000             76020.000000   \nmean                  3.559130                 6.472698   \nstd                  93.155749               153.737066   \nmin                   0.000000                 0.000000   \n25%                   0.000000                 0.000000   \n50%                   0.000000                 0.000000   \n75%                   0.000000                 0.000000   \nmax                8237.820000             11073.570000   \n\n       imp_op_var40_efect_ult1  imp_op_var40_efect_ult3  imp_op_var40_ult1  \\\ncount             76020.000000             76020.000000       76020.000000   \nmean                  0.412946                 0.567352           3.160715   \nstd                  30.604864                36.513513          95.268204   \nmin                   0.000000                 0.000000           0.000000   \n25%                   0.000000                 0.000000           0.000000   \n50%                   0.000000                 0.000000           0.000000   \n75%                   0.000000                 0.000000           0.000000   \nmax                6600.000000              6600.000000        8237.820000   \n\n       ...  saldo_medio_var33_hace2  saldo_medio_var33_hace3  \\\ncount  ...             76020.000000             76020.000000   \nmean   ...                 7.935824                 1.365146   \nstd    ...               455.887218               113.959637   \nmin    ...                 0.000000                 0.000000   \n25%    ...                 0.000000                 0.000000   \n50%    ...                 0.000000                 0.000000   \n75%    ...                 0.000000                 0.000000   \nmax    ...             50003.880000             20385.720000   \n\n       saldo_medio_var33_ult1  saldo_medio_var33_ult3  \\\ncount            76020.000000            76020.000000   \nmean                12.215580                8.784074   \nstd                783.207399              538.439211   \nmin                  0.000000                0.000000   \n25%                  0.000000                0.000000   \n50%                  0.000000                0.000000   \n75%                  0.000000                0.000000   \nmax             138831.630000            91778.730000   \n\n       saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\ncount             76020.000000             76020.000000   \nmean                 31.505324                 1.858575   \nstd                2013.125393               147.786584   \nmin                   0.000000                 0.000000   \n25%                   0.000000                 0.000000   \n50%                   0.000000                 0.000000   \n75%                   0.000000                 0.000000   \nmax              438329.220000             24650.010000   \n\n       saldo_medio_var44_ult1  saldo_medio_var44_ult3         var38  \\\ncount            76020.000000            76020.000000  7.602000e+04   \nmean                76.026165               56.614351  1.172358e+05   \nstd               4040.337842             2852.579397  1.826646e+05   \nmin                  0.000000                0.000000  5.163750e+03   \n25%                  0.000000                0.000000  6.787061e+04   \n50%                  0.000000                0.000000  1.064092e+05   \n75%                  0.000000                0.000000  1.187563e+05   \nmax             681462.900000           397884.300000  2.203474e+07   \n\n             TARGET  \ncount  76020.000000  \nmean       0.039569  \nstd        0.194945  \nmin        0.000000  \n25%        0.000000  \n50%        0.000000  \n75%        0.000000  \nmax        1.000000  \n\n[8 rows x 370 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var3</th>\n      <th>var15</th>\n      <th>imp_ent_var16_ult1</th>\n      <th>imp_op_var39_comer_ult1</th>\n      <th>imp_op_var39_comer_ult3</th>\n      <th>imp_op_var40_comer_ult1</th>\n      <th>imp_op_var40_comer_ult3</th>\n      <th>imp_op_var40_efect_ult1</th>\n      <th>imp_op_var40_efect_ult3</th>\n      <th>imp_op_var40_ult1</th>\n      <th>...</th>\n      <th>saldo_medio_var33_hace2</th>\n      <th>saldo_medio_var33_hace3</th>\n      <th>saldo_medio_var33_ult1</th>\n      <th>saldo_medio_var33_ult3</th>\n      <th>saldo_medio_var44_hace2</th>\n      <th>saldo_medio_var44_hace3</th>\n      <th>saldo_medio_var44_ult1</th>\n      <th>saldo_medio_var44_ult3</th>\n      <th>var38</th>\n      <th>TARGET</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>...</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>76020.000000</td>\n      <td>7.602000e+04</td>\n      <td>76020.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-1523.199277</td>\n      <td>33.212865</td>\n      <td>86.208265</td>\n      <td>72.363067</td>\n      <td>119.529632</td>\n      <td>3.559130</td>\n      <td>6.472698</td>\n      <td>0.412946</td>\n      <td>0.567352</td>\n      <td>3.160715</td>\n      <td>...</td>\n      <td>7.935824</td>\n      <td>1.365146</td>\n      <td>12.215580</td>\n      <td>8.784074</td>\n      <td>31.505324</td>\n      <td>1.858575</td>\n      <td>76.026165</td>\n      <td>56.614351</td>\n      <td>1.172358e+05</td>\n      <td>0.039569</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>39033.462364</td>\n      <td>12.956486</td>\n      <td>1614.757313</td>\n      <td>339.315831</td>\n      <td>546.266294</td>\n      <td>93.155749</td>\n      <td>153.737066</td>\n      <td>30.604864</td>\n      <td>36.513513</td>\n      <td>95.268204</td>\n      <td>...</td>\n      <td>455.887218</td>\n      <td>113.959637</td>\n      <td>783.207399</td>\n      <td>538.439211</td>\n      <td>2013.125393</td>\n      <td>147.786584</td>\n      <td>4040.337842</td>\n      <td>2852.579397</td>\n      <td>1.826646e+05</td>\n      <td>0.194945</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-999999.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.163750e+03</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n      <td>23.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.787061e+04</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.000000</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.064092e+05</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.000000</td>\n      <td>40.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.187563e+05</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>238.000000</td>\n      <td>105.000000</td>\n      <td>210000.000000</td>\n      <td>12888.030000</td>\n      <td>21024.810000</td>\n      <td>8237.820000</td>\n      <td>11073.570000</td>\n      <td>6600.000000</td>\n      <td>6600.000000</td>\n      <td>8237.820000</td>\n      <td>...</td>\n      <td>50003.880000</td>\n      <td>20385.720000</td>\n      <td>138831.630000</td>\n      <td>91778.730000</td>\n      <td>438329.220000</td>\n      <td>24650.010000</td>\n      <td>681462.900000</td>\n      <td>397884.300000</td>\n      <td>2.203474e+07</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 370 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# summary: COMPLETE workflow\n\n# Replace -999999 in var3 column with most common value 2 \n# See https://www.kaggle.com/cast42/santander-customer-satisfaction/debugging-var3-999999\n# for details\ntraining = training.replace(-999999,2)\n\n\n# Replace 9999999999 with NaN\n# See https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19291/data-dictionary/111360#post111360\n# training = training.replace(9999999999, np.nan)\n# training.dropna(inplace=True)\n# Leads to validation_0-auc:0.839577\n\nX = training.iloc[:,:-1]\ny = training.TARGET\n\n# Add zeros per row as extra feature\nX['n0'] = (X == 0).sum(axis=1)\n# # Add log of var38\n# X['logvar38'] = X['var38'].map(np.log1p)\n# # Encode var36 as category\n# X['var36'] = X['var36'].astype('category')\n# X = pd.get_dummies(X)\n\n# Add PCA components as features\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\n\nX_normalized = normalize(X, axis=0)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_normalized)\nX['PCA1'] = X_pca[:,0]\nX['PCA2'] = X_pca[:,1]\n\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_classif,chi2\nfrom sklearn.preprocessing import Binarizer, scale\n\np = 86 # 308 features validation_1-auc:0.848039\np = 80 # 284 features validation_1-auc:0.848414\np = 77 # 267 features validation_1-auc:0.848000\np = 75 # 261 features validation_1-auc:0.848642\n# p = 73 # 257 features validation_1-auc:0.848338\n# p = 70 # 259 features validation_1-auc:0.848588\n# p = 69 # 238 features validation_1-auc:0.848547\n# p = 67 # 247 features validation_1-auc:0.847925\n# p = 65 # 240 features validation_1-auc:0.846769\n# p = 60 # 222 features validation_1-auc:0.848581\n\nX_bin = Binarizer().fit_transform(scale(X))\nselectChi2 = SelectPercentile(chi2, percentile=p).fit(X_bin, y)\nselectF_classif = SelectPercentile(f_classif, percentile=p).fit(X, y)\n\nchi2_selected = selectChi2.get_support()\nchi2_selected_features = [ f for i,f in enumerate(X.columns) if chi2_selected[i]]\nprint('Chi2 selected {} features {}.'.format(chi2_selected.sum(),\n   chi2_selected_features))\nf_classif_selected = selectF_classif.get_support()\nf_classif_selected_features = [ f for i,f in enumerate(X.columns) if f_classif_selected[i]]\nprint('F_classif selected {} features {}.'.format(f_classif_selected.sum(),\n   f_classif_selected_features))\nselected = chi2_selected & f_classif_selected\nprint('Chi2 & F_classif selected {} features'.format(selected.sum()))\nfeatures = [ f for f,s in zip(X.columns, selected) if s]\nprint (features)\n\nX_sel = X[features]\n\nX_train, X_test, y_train, y_test = \\\n  cross_validation.train_test_split(X_sel, y, random_state=1301, stratify=y, test_size=0.4)\n\n# xgboost parameter tuning with p = 75\n# recipe: https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/19083/best-practices-for-parameter-tuning-on-models/108783#post108783\n\nratio = float(np.sum(y == 1)) / np.sum(y==0)\n# Initial parameters for the parameter exploration\n# clf = xgb.XGBClassifier(missing=9999999999,\n#                 max_depth = 10,\n#                 n_estimators=1000,\n#                 learning_rate=0.1, \n#                 nthread=4,\n#                 subsample=1.0,\n#                 colsample_bytree=0.5,\n#                 min_child_weight = 5,\n#                 scale_pos_weight = ratio,\n#                 seed=4242)\n\n# gives : validation_1-auc:0.845644\n# max_depth=8 -> validation_1-auc:0.846341\n# max_depth=6 -> validation_1-auc:0.845738\n# max_depth=7 -> validation_1-auc:0.846504\n# subsample=0.8 -> validation_1-auc:0.844440\n# subsample=0.9 -> validation_1-auc:0.844746\n# subsample=1.0,  min_child_weight=8 -> validation_1-auc:0.843393\n# min_child_weight=3 -> validation_1-auc:0.848534\n# min_child_weight=1 -> validation_1-auc:0.846311\n# min_child_weight=4 -> validation_1-auc:0.847994\n# min_child_weight=2 -> validation_1-auc:0.847934\n# min_child_weight=3, colsample_bytree=0.3 -> validation_1-auc:0.847498\n# colsample_bytree=0.7 -> validation_1-auc:0.846984\n# colsample_bytree=0.6 -> validation_1-auc:0.847856\n# colsample_bytree=0.5, learning_rate=0.05 -> validation_1-auc:0.847347\n# max_depth=8 -> validation_1-auc:0.847352\n# learning_rate = 0.07 -> validation_1-auc:0.847432\n# learning_rate = 0.2 -> validation_1-auc:0.846444\n# learning_rate = 0.15 -> validation_1-auc:0.846889\n# learning_rate = 0.09 -> validation_1-auc:0.846680\n# learning_rate = 0.1 -> validation_1-auc:0.847432\n# max_depth=7 -> validation_1-auc:0.848534\n# learning_rate = 0.05 -> validation_1-auc:0.847347\n# \n\nclf = xgb.XGBClassifier(missing=9999999999,\n                max_depth = 5,\n                n_estimators=1000,\n                learning_rate=0.1, \n                nthread=4,\n                subsample=1.0,\n                colsample_bytree=0.5,\n                min_child_weight = 3,\n                scale_pos_weight = ratio,\n                reg_alpha=0.03,\n                seed=1301)\n                \nclf.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"auc\",\n        eval_set=[(X_train, y_train), (X_test, y_test)])\n        \nprint('Overall AUC:', roc_auc_score(y, clf.predict_proba(X_sel, ntree_limit=clf.best_iteration)[:,1]))\n\ntest['n0'] = (test == 0).sum(axis=1)\n# test['logvar38'] = test['var38'].map(np.log1p)\n# # Encode var36 as category\n# test['var36'] = test['var36'].astype('category')\n# test = pd.get_dummies(test)\ntest_normalized = normalize(test, axis=0)\npca = PCA(n_components=2)\ntest_pca = pca.fit_transform(test_normalized)\ntest['PCA1'] = test_pca[:,0]\ntest['PCA2'] = test_pca[:,1]\nsel_test = test[features]    \ny_pred = clf.predict_proba(sel_test, ntree_limit=clf.best_iteration)\n\nsubmission = pd.DataFrame({\"ID\":test.index, \"TARGET\":y_pred[:,1]})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nmapFeat = dict(zip([\"f\"+str(i) for i in range(len(features))],features))\nts = pd.Series(clf.booster().get_fscore())\n#ts.index = ts.reset_index()['index'].map(mapFeat)\nts.sort_values()[-15:].plot(kind=\"barh\", title=(\"features importance\"))\n\nfeatp = ts.sort_values()[-15:].plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('XGBoost Feature Importance')\nfig_featp = featp.get_figure()\nfig_featp.savefig('feature_importance_xgb.png', bbox_inches='tight', pad_inches=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}